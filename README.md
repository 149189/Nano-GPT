# ğŸš€ Nano-GPT: Transformer-Based Language Model

An interactive Transformer language model trained on the Tiny [Shakespeare dataset](https://www.kaggle.com/datasets/debdeepsanyal/tiny-shakespeare-dataset). ğŸ“œ

Inspired by Andrej Karpathy, this project helped me understand the core concepts behind Transformers, like self-attention and sequence generation. Iâ€™m now working on scaling this up to create a GPT-2 clone! ğŸš€

---

## ğŸ“˜ What I Built

- **Self-Attention Mechanism:** Helps the model focus on relevant tokens in the sequence.
- **Multi-Head Attention:** Captures diverse relationships between tokens for richer understanding.
- **Positional Embeddings:** Gives the model a sense of token order, crucial for sequences.
- **Layer Normalization:** Stabilizes and accelerates training for better convergence.
- **Feedforward Neural Network:** Processes attention outputs to extract complex features.
- **Token and Positional Embeddings:** Converts characters into meaningful vector representations.
- **Greedy & Multinomial Sampling:** Enables flexible and creative text generation.

---

## ğŸ› ï¸ Model Details

- ğŸ§  **Parameters:** ~3 Million  
- ğŸ“š **Dataset:** Tiny Shakespeare (~1MB of Shakespeareâ€™s works)  
- âš¡ **Training Split:** 90% training, 10% validation  
- ğŸ“Š **Loss Estimation:** Robust evaluation on both train and validation sets  

After training, the model can generate Shakespeare-like text â€” itâ€™s incredible to see how it captures the style and rhythm of classic literature! âœ¨

---

## ğŸš€ Try It Out!

- ğŸ”— **[Live Demo on Hugging Face](https://huggingface.co/spaces/kaustubhr4/Nano_gpt)**  
- ğŸ”— **[Explore the Code on Kaggle](https://www.kaggle.com/code/kaustubhratwadkar/new-nano-gpt)**

---

## ğŸ’¡ Whatâ€™s Next?

Iâ€™m working on scaling this up into a GPT-2 clone, and Iâ€™m exploring optimizations like rotary embeddings and flash attention. If you have ideas or suggestions, Iâ€™d love to hear them! Letâ€™s keep learning and building together. ğŸš€

---

## ğŸ¤ Acknowledgments

A huge shoutout to **Andrej Karpathy** for making complex concepts like self-attention, multi-head attention, and layer normalization so understandable â€” your teachings inspired this project! ğŸ™Œ

#Demo_pic
![image](https://github.com/user-attachments/assets/48707d27-2fd6-4f1d-b861-f0532d42b56e)

